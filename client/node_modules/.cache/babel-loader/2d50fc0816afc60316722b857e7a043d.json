{"ast":null,"code":"'use strict';\n\nvar _regeneratorRuntime = require(\"D:/Poject-main/Poject-main/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/regenerator\");\n\nvar _asyncToGenerator = require(\"D:/Poject-main/Poject-main/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/asyncToGenerator\");\n\nvar _require = require('ipld-dag-pb'),\n    DAGNode = _require.DAGNode;\n\nvar Bucket = require('hamt-sharding/src/bucket');\n\nvar DirSharded = require('ipfs-unixfs-importer/src/dir-sharded');\n\nvar log = require('debug')('ipfs:mfs:core:utils:hamt-utils');\n\nvar UnixFS = require('ipfs-unixfs');\n\nvar mc = require('multicodec');\n\nvar mh = require('multihashes');\n\nvar last = require('async-iterator-last');\n\nvar updateHamtDirectory = /*#__PURE__*/function () {\n  var _ref = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee(context, links, bucket, options) {\n    var data, dir, format, hashAlg, parent, cid;\n    return _regeneratorRuntime.wrap(function _callee$(_context) {\n      while (1) {\n        switch (_context.prev = _context.next) {\n          case 0:\n            // update parent with new bit field\n            data = Buffer.from(bucket._children.bitField().reverse());\n            dir = new UnixFS('hamt-sharded-directory', data);\n            dir.fanout = bucket.tableSize();\n            dir.hashType = DirSharded.hashFn.code;\n            format = mc[options.format.toUpperCase().replace(/-/g, '_')];\n            hashAlg = mh.names[options.hashAlg];\n            parent = DAGNode.create(dir.marshal(), links);\n            _context.next = 9;\n            return context.ipld.put(parent, format, {\n              cidVersion: options.cidVersion,\n              hashAlg: hashAlg,\n              hashOnly: !options.flush\n            });\n\n          case 9:\n            cid = _context.sent;\n            return _context.abrupt(\"return\", {\n              node: parent,\n              cid: cid\n            });\n\n          case 11:\n          case \"end\":\n            return _context.stop();\n        }\n      }\n    }, _callee);\n  }));\n\n  return function updateHamtDirectory(_x, _x2, _x3, _x4) {\n    return _ref.apply(this, arguments);\n  };\n}();\n\nvar recreateHamtLevel = /*#__PURE__*/function () {\n  var _ref2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2(links, rootBucket, parentBucket, positionAtParent) {\n    var bucket;\n    return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n      while (1) {\n        switch (_context2.prev = _context2.next) {\n          case 0:\n            // recreate this level of the HAMT\n            bucket = new Bucket({\n              hashFn: DirSharded.hashFn,\n              hash: parentBucket ? parentBucket._options.hash : undefined\n            }, parentBucket, positionAtParent);\n\n            if (parentBucket) {\n              parentBucket._putObjectAt(positionAtParent, bucket);\n            }\n\n            _context2.next = 4;\n            return addLinksToHamtBucket(links, bucket, rootBucket);\n\n          case 4:\n            return _context2.abrupt(\"return\", bucket);\n\n          case 5:\n          case \"end\":\n            return _context2.stop();\n        }\n      }\n    }, _callee2);\n  }));\n\n  return function recreateHamtLevel(_x5, _x6, _x7, _x8) {\n    return _ref2.apply(this, arguments);\n  };\n}();\n\nvar addLinksToHamtBucket = /*#__PURE__*/function () {\n  var _ref3 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3(links, bucket, rootBucket) {\n    return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n      while (1) {\n        switch (_context3.prev = _context3.next) {\n          case 0:\n            _context3.next = 2;\n            return Promise.all(links.map(function (link) {\n              if (link.Name.length === 2) {\n                var pos = parseInt(link.Name, 16);\n\n                bucket._putObjectAt(pos, new Bucket({\n                  hashFn: DirSharded.hashFn\n                }, bucket, pos));\n\n                return Promise.resolve();\n              }\n\n              return (rootBucket || bucket).put(link.Name.substring(2), {\n                size: link.TSize,\n                cid: link.Hash\n              });\n            }));\n\n          case 2:\n          case \"end\":\n            return _context3.stop();\n        }\n      }\n    }, _callee3);\n  }));\n\n  return function addLinksToHamtBucket(_x9, _x10, _x11) {\n    return _ref3.apply(this, arguments);\n  };\n}();\n\nvar toPrefix = function toPrefix(position) {\n  return position.toString('16').toUpperCase().padStart(2, '0').substring(0, 2);\n};\n\nvar generatePath = /*#__PURE__*/function () {\n  var _ref4 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee4(context, fileName, rootNode) {\n    var rootBucket, position, path, currentBucket, _loop, i, _ret;\n\n    return _regeneratorRuntime.wrap(function _callee4$(_context5) {\n      while (1) {\n        switch (_context5.prev = _context5.next) {\n          case 0:\n            _context5.next = 2;\n            return recreateHamtLevel(rootNode.Links, null, null, null);\n\n          case 2:\n            rootBucket = _context5.sent;\n            _context5.next = 5;\n            return rootBucket._findNewBucketAndPos(fileName);\n\n          case 5:\n            position = _context5.sent;\n            // the path to the root bucket\n            path = [{\n              bucket: position.bucket,\n              prefix: toPrefix(position.pos)\n            }];\n            currentBucket = position.bucket;\n\n            while (currentBucket !== rootBucket) {\n              path.push({\n                bucket: currentBucket,\n                prefix: toPrefix(currentBucket._posAtParent)\n              });\n              currentBucket = currentBucket._parent;\n            }\n\n            path.reverse();\n            path[0].node = rootNode; // load DAGNode for each path segment\n\n            _loop = /*#__PURE__*/_regeneratorRuntime.mark(function _loop(i) {\n              var segment, link, node, _position, nextSegment;\n\n              return _regeneratorRuntime.wrap(function _loop$(_context4) {\n                while (1) {\n                  switch (_context4.prev = _context4.next) {\n                    case 0:\n                      segment = path[i]; // find prefix in links\n\n                      link = segment.node.Links.filter(function (link) {\n                        return link.Name.substring(0, 2) === segment.prefix;\n                      }).pop(); // entry was not in shard\n\n                      if (link) {\n                        _context4.next = 5;\n                        break;\n                      }\n\n                      // reached bottom of tree, file will be added to the current bucket\n                      log(\"Link \".concat(segment.prefix).concat(fileName, \" will be added\")); // return path\n\n                      return _context4.abrupt(\"return\", \"continue\");\n\n                    case 5:\n                      if (!(link.Name === \"\".concat(segment.prefix).concat(fileName))) {\n                        _context4.next = 8;\n                        break;\n                      }\n\n                      log(\"Link \".concat(segment.prefix).concat(fileName, \" will be replaced\")); // file already existed, file will be added to the current bucket\n                      // return path\n\n                      return _context4.abrupt(\"return\", \"continue\");\n\n                    case 8:\n                      // found subshard\n                      log(\"Found subshard \".concat(segment.prefix));\n                      _context4.next = 11;\n                      return context.ipld.get(link.Hash);\n\n                    case 11:\n                      node = _context4.sent;\n\n                      if (path[i + 1]) {\n                        _context4.next = 21;\n                        break;\n                      }\n\n                      log(\"Loaded new subshard \".concat(segment.prefix));\n                      _context4.next = 16;\n                      return recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n\n                    case 16:\n                      _context4.next = 18;\n                      return rootBucket._findNewBucketAndPos(fileName);\n\n                    case 18:\n                      _position = _context4.sent;\n                      // i--\n                      path.push({\n                        bucket: _position.bucket,\n                        prefix: toPrefix(_position.pos),\n                        node: node\n                      });\n                      return _context4.abrupt(\"return\", \"continue\");\n\n                    case 21:\n                      nextSegment = path[i + 1]; // add intermediate links to bucket\n\n                      _context4.next = 24;\n                      return addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket);\n\n                    case 24:\n                      nextSegment.node = node;\n\n                    case 25:\n                    case \"end\":\n                      return _context4.stop();\n                  }\n                }\n              }, _loop);\n            });\n            i = 0;\n\n          case 13:\n            if (!(i < path.length)) {\n              _context5.next = 21;\n              break;\n            }\n\n            return _context5.delegateYield(_loop(i), \"t0\", 15);\n\n          case 15:\n            _ret = _context5.t0;\n\n            if (!(_ret === \"continue\")) {\n              _context5.next = 18;\n              break;\n            }\n\n            return _context5.abrupt(\"continue\", 18);\n\n          case 18:\n            i++;\n            _context5.next = 13;\n            break;\n\n          case 21:\n            _context5.next = 23;\n            return rootBucket.put(fileName, true);\n\n          case 23:\n            path.reverse();\n            return _context5.abrupt(\"return\", {\n              rootBucket: rootBucket,\n              path: path\n            });\n\n          case 25:\n          case \"end\":\n            return _context5.stop();\n        }\n      }\n    }, _callee4);\n  }));\n\n  return function generatePath(_x12, _x13, _x14) {\n    return _ref4.apply(this, arguments);\n  };\n}();\n\nvar createShard = /*#__PURE__*/function () {\n  var _ref5 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee5(context, contents, options) {\n    var shard, i;\n    return _regeneratorRuntime.wrap(function _callee5$(_context6) {\n      while (1) {\n        switch (_context6.prev = _context6.next) {\n          case 0:\n            shard = new DirSharded({\n              root: true,\n              dir: true,\n              parent: null,\n              parentKey: null,\n              path: '',\n              dirty: true,\n              flat: false\n            }, options);\n            i = 0;\n\n          case 2:\n            if (!(i < contents.length)) {\n              _context6.next = 8;\n              break;\n            }\n\n            _context6.next = 5;\n            return shard._bucket.put(contents[i].name, {\n              size: contents[i].size,\n              cid: contents[i].cid\n            });\n\n          case 5:\n            i++;\n            _context6.next = 2;\n            break;\n\n          case 8:\n            return _context6.abrupt(\"return\", last(shard.flush('', context.ipld, null)));\n\n          case 9:\n          case \"end\":\n            return _context6.stop();\n        }\n      }\n    }, _callee5);\n  }));\n\n  return function createShard(_x15, _x16, _x17) {\n    return _ref5.apply(this, arguments);\n  };\n}();\n\nmodule.exports = {\n  generatePath: generatePath,\n  updateHamtDirectory: updateHamtDirectory,\n  recreateHamtLevel: recreateHamtLevel,\n  addLinksToHamtBucket: addLinksToHamtBucket,\n  toPrefix: toPrefix,\n  createShard: createShard\n};","map":{"version":3,"sources":["D:/Poject-main/Poject-main/client/node_modules/ipfs-mfs/src/core/utils/hamt-utils.js"],"names":["require","DAGNode","Bucket","DirSharded","log","UnixFS","mc","mh","last","updateHamtDirectory","context","links","bucket","options","data","Buffer","from","_children","bitField","reverse","dir","fanout","tableSize","hashType","hashFn","code","format","toUpperCase","replace","hashAlg","names","parent","create","marshal","ipld","put","cidVersion","hashOnly","flush","cid","node","recreateHamtLevel","rootBucket","parentBucket","positionAtParent","hash","_options","undefined","_putObjectAt","addLinksToHamtBucket","Promise","all","map","link","Name","length","pos","parseInt","resolve","substring","size","TSize","Hash","toPrefix","position","toString","padStart","generatePath","fileName","rootNode","Links","_findNewBucketAndPos","path","prefix","currentBucket","push","_posAtParent","_parent","i","segment","filter","pop","get","nextSegment","createShard","contents","shard","root","parentKey","dirty","flat","_bucket","name","module","exports"],"mappings":"AAAA;;;;;;AAEA,eAEIA,OAAO,CAAC,aAAD,CAFX;AAAA,IACEC,OADF,YACEA,OADF;;AAGA,IAAMC,MAAM,GAAGF,OAAO,CAAC,0BAAD,CAAtB;;AACA,IAAMG,UAAU,GAAGH,OAAO,CAAC,sCAAD,CAA1B;;AACA,IAAMI,GAAG,GAAGJ,OAAO,CAAC,OAAD,CAAP,CAAiB,gCAAjB,CAAZ;;AACA,IAAMK,MAAM,GAAGL,OAAO,CAAC,aAAD,CAAtB;;AACA,IAAMM,EAAE,GAAGN,OAAO,CAAC,YAAD,CAAlB;;AACA,IAAMO,EAAE,GAAGP,OAAO,CAAC,aAAD,CAAlB;;AACA,IAAMQ,IAAI,GAAGR,OAAO,CAAC,qBAAD,CAApB;;AAEA,IAAMS,mBAAmB;AAAA,sEAAG,iBAAOC,OAAP,EAAgBC,KAAhB,EAAuBC,MAAvB,EAA+BC,OAA/B;AAAA;AAAA;AAAA;AAAA;AAAA;AAC1B;AACMC,YAAAA,IAFoB,GAEbC,MAAM,CAACC,IAAP,CAAYJ,MAAM,CAACK,SAAP,CAAiBC,QAAjB,GAA4BC,OAA5B,EAAZ,CAFa;AAGpBC,YAAAA,GAHoB,GAGd,IAAIf,MAAJ,CAAW,wBAAX,EAAqCS,IAArC,CAHc;AAI1BM,YAAAA,GAAG,CAACC,MAAJ,GAAaT,MAAM,CAACU,SAAP,EAAb;AACAF,YAAAA,GAAG,CAACG,QAAJ,GAAepB,UAAU,CAACqB,MAAX,CAAkBC,IAAjC;AAEMC,YAAAA,MAPoB,GAOXpB,EAAE,CAACO,OAAO,CAACa,MAAR,CAAeC,WAAf,GAA6BC,OAA7B,CAAqC,IAArC,EAA2C,GAA3C,CAAD,CAPS;AAQpBC,YAAAA,OARoB,GAQVtB,EAAE,CAACuB,KAAH,CAASjB,OAAO,CAACgB,OAAjB,CARU;AAUpBE,YAAAA,MAVoB,GAUX9B,OAAO,CAAC+B,MAAR,CAAeZ,GAAG,CAACa,OAAJ,EAAf,EAA8BtB,KAA9B,CAVW;AAAA;AAAA,mBAWRD,OAAO,CAACwB,IAAR,CAAaC,GAAb,CAAiBJ,MAAjB,EAAyBL,MAAzB,EAAiC;AACjDU,cAAAA,UAAU,EAAEvB,OAAO,CAACuB,UAD6B;AAEjDP,cAAAA,OAAO,EAAPA,OAFiD;AAGjDQ,cAAAA,QAAQ,EAAE,CAACxB,OAAO,CAACyB;AAH8B,aAAjC,CAXQ;;AAAA;AAWpBC,YAAAA,GAXoB;AAAA,6CAiBnB;AACLC,cAAAA,IAAI,EAAET,MADD;AAELQ,cAAAA,GAAG,EAAHA;AAFK,aAjBmB;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,GAAH;;AAAA,kBAAnB9B,mBAAmB;AAAA;AAAA;AAAA,GAAzB;;AAuBA,IAAMgC,iBAAiB;AAAA,uEAAG,kBAAO9B,KAAP,EAAc+B,UAAd,EAA0BC,YAA1B,EAAwCC,gBAAxC;AAAA;AAAA;AAAA;AAAA;AAAA;AACxB;AACMhC,YAAAA,MAFkB,GAET,IAAIV,MAAJ,CAAW;AACxBsB,cAAAA,MAAM,EAAErB,UAAU,CAACqB,MADK;AAExBqB,cAAAA,IAAI,EAAEF,YAAY,GAAGA,YAAY,CAACG,QAAb,CAAsBD,IAAzB,GAAgCE;AAF1B,aAAX,EAGZJ,YAHY,EAGEC,gBAHF,CAFS;;AAOxB,gBAAID,YAAJ,EAAkB;AAChBA,cAAAA,YAAY,CAACK,YAAb,CAA0BJ,gBAA1B,EAA4ChC,MAA5C;AACD;;AATuB;AAAA,mBAWlBqC,oBAAoB,CAACtC,KAAD,EAAQC,MAAR,EAAgB8B,UAAhB,CAXF;;AAAA;AAAA,8CAajB9B,MAbiB;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,GAAH;;AAAA,kBAAjB6B,iBAAiB;AAAA;AAAA;AAAA,GAAvB;;AAgBA,IAAMQ,oBAAoB;AAAA,uEAAG,kBAAOtC,KAAP,EAAcC,MAAd,EAAsB8B,UAAtB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mBACrBQ,OAAO,CAACC,GAAR,CACJxC,KAAK,CAACyC,GAAN,CAAU,UAAAC,IAAI,EAAI;AAChB,kBAAIA,IAAI,CAACC,IAAL,CAAUC,MAAV,KAAqB,CAAzB,EAA4B;AAC1B,oBAAMC,GAAG,GAAGC,QAAQ,CAACJ,IAAI,CAACC,IAAN,EAAY,EAAZ,CAApB;;AAEA1C,gBAAAA,MAAM,CAACoC,YAAP,CAAoBQ,GAApB,EAAyB,IAAItD,MAAJ,CAAW;AAClCsB,kBAAAA,MAAM,EAAErB,UAAU,CAACqB;AADe,iBAAX,EAEtBZ,MAFsB,EAEd4C,GAFc,CAAzB;;AAIA,uBAAON,OAAO,CAACQ,OAAR,EAAP;AACD;;AAED,qBAAO,CAAChB,UAAU,IAAI9B,MAAf,EAAuBuB,GAAvB,CAA2BkB,IAAI,CAACC,IAAL,CAAUK,SAAV,CAAoB,CAApB,CAA3B,EAAmD;AACxDC,gBAAAA,IAAI,EAAEP,IAAI,CAACQ,KAD6C;AAExDtB,gBAAAA,GAAG,EAAEc,IAAI,CAACS;AAF8C,eAAnD,CAAP;AAID,aAfD,CADI,CADqB;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,GAAH;;AAAA,kBAApBb,oBAAoB;AAAA;AAAA;AAAA,GAA1B;;AAqBA,IAAMc,QAAQ,GAAG,SAAXA,QAAW,CAACC,QAAD,EAAc;AAC7B,SAAOA,QAAQ,CACZC,QADI,CACK,IADL,EAEJtC,WAFI,GAGJuC,QAHI,CAGK,CAHL,EAGQ,GAHR,EAIJP,SAJI,CAIM,CAJN,EAIS,CAJT,CAAP;AAKD,CAND;;AAQA,IAAMQ,YAAY;AAAA,uEAAG,kBAAOzD,OAAP,EAAgB0D,QAAhB,EAA0BC,QAA1B;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mBAEM5B,iBAAiB,CAAC4B,QAAQ,CAACC,KAAV,EAAiB,IAAjB,EAAuB,IAAvB,EAA6B,IAA7B,CAFvB;;AAAA;AAEb5B,YAAAA,UAFa;AAAA;AAAA,mBAGIA,UAAU,CAAC6B,oBAAX,CAAgCH,QAAhC,CAHJ;;AAAA;AAGbJ,YAAAA,QAHa;AAKnB;AACIQ,YAAAA,IANe,GAMR,CAAC;AACV5D,cAAAA,MAAM,EAAEoD,QAAQ,CAACpD,MADP;AAEV6D,cAAAA,MAAM,EAAEV,QAAQ,CAACC,QAAQ,CAACR,GAAV;AAFN,aAAD,CANQ;AAUfkB,YAAAA,aAVe,GAUCV,QAAQ,CAACpD,MAVV;;AAYnB,mBAAO8D,aAAa,KAAKhC,UAAzB,EAAqC;AACnC8B,cAAAA,IAAI,CAACG,IAAL,CAAU;AACR/D,gBAAAA,MAAM,EAAE8D,aADA;AAERD,gBAAAA,MAAM,EAAEV,QAAQ,CAACW,aAAa,CAACE,YAAf;AAFR,eAAV;AAKAF,cAAAA,aAAa,GAAGA,aAAa,CAACG,OAA9B;AACD;;AAEDL,YAAAA,IAAI,CAACrD,OAAL;AACAqD,YAAAA,IAAI,CAAC,CAAD,CAAJ,CAAQhC,IAAR,GAAe6B,QAAf,CAtBmB,CAwBnB;;AAxBmB,yEAyBVS,CAzBU;AAAA;;AAAA;AAAA;AAAA;AAAA;AA0BXC,sBAAAA,OA1BW,GA0BDP,IAAI,CAACM,CAAD,CA1BH,EA4BjB;;AACMzB,sBAAAA,IA7BW,GA6BJ0B,OAAO,CAACvC,IAAR,CAAa8B,KAAb,CACVU,MADU,CACH,UAAA3B,IAAI;AAAA,+BAAIA,IAAI,CAACC,IAAL,CAAUK,SAAV,CAAoB,CAApB,EAAuB,CAAvB,MAA8BoB,OAAO,CAACN,MAA1C;AAAA,uBADD,EAEVQ,GAFU,EA7BI,EAiCjB;;AAjCiB,0BAkCZ5B,IAlCY;AAAA;AAAA;AAAA;;AAmCf;AACAjD,sBAAAA,GAAG,gBAAS2E,OAAO,CAACN,MAAjB,SAA0BL,QAA1B,oBAAH,CApCe,CAqCf;;AArCe;;AAAA;AAAA,4BA0Cbf,IAAI,CAACC,IAAL,eAAiByB,OAAO,CAACN,MAAzB,SAAkCL,QAAlC,CA1Ca;AAAA;AAAA;AAAA;;AA2CfhE,sBAAAA,GAAG,gBAAS2E,OAAO,CAACN,MAAjB,SAA0BL,QAA1B,uBAAH,CA3Ce,CA4Cf;AACA;;AA7Ce;;AAAA;AAiDjB;AACAhE,sBAAAA,GAAG,0BAAmB2E,OAAO,CAACN,MAA3B,EAAH;AAlDiB;AAAA,6BAmDE/D,OAAO,CAACwB,IAAR,CAAagD,GAAb,CAAiB7B,IAAI,CAACS,IAAtB,CAnDF;;AAAA;AAmDXtB,sBAAAA,IAnDW;;AAAA,0BAsDZgC,IAAI,CAACM,CAAC,GAAG,CAAL,CAtDQ;AAAA;AAAA;AAAA;;AAuDf1E,sBAAAA,GAAG,+BAAwB2E,OAAO,CAACN,MAAhC,EAAH;AAvDe;AAAA,6BAyDThC,iBAAiB,CAACD,IAAI,CAAC8B,KAAN,EAAa5B,UAAb,EAAyBqC,OAAO,CAACnE,MAAjC,EAAyC6C,QAAQ,CAACsB,OAAO,CAACN,MAAT,EAAiB,EAAjB,CAAjD,CAzDR;;AAAA;AAAA;AAAA,6BA0DQ/B,UAAU,CAAC6B,oBAAX,CAAgCH,QAAhC,CA1DR;;AAAA;AA0DTJ,sBAAAA,SA1DS;AA4Df;AACAQ,sBAAAA,IAAI,CAACG,IAAL,CAAU;AACR/D,wBAAAA,MAAM,EAAEoD,SAAQ,CAACpD,MADT;AAER6D,wBAAAA,MAAM,EAAEV,QAAQ,CAACC,SAAQ,CAACR,GAAV,CAFR;AAGRhB,wBAAAA,IAAI,EAAEA;AAHE,uBAAV;AA7De;;AAAA;AAsEX2C,sBAAAA,WAtEW,GAsEGX,IAAI,CAACM,CAAC,GAAG,CAAL,CAtEP,EAwEjB;;AAxEiB;AAAA,6BAyEX7B,oBAAoB,CAACT,IAAI,CAAC8B,KAAN,EAAaa,WAAW,CAACvE,MAAzB,EAAiC8B,UAAjC,CAzET;;AAAA;AA2EjByC,sBAAAA,WAAW,CAAC3C,IAAZ,GAAmBA,IAAnB;;AA3EiB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAyBVsC,YAAAA,CAzBU,GAyBN,CAzBM;;AAAA;AAAA,kBAyBHA,CAAC,GAAGN,IAAI,CAACjB,MAzBN;AAAA;AAAA;AAAA;;AAAA,iDAyBVuB,CAzBU;;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;;AAAA;;AAAA;AAyBcA,YAAAA,CAAC,EAzBf;AAAA;AAAA;;AAAA;AAAA;AAAA,mBA8EbpC,UAAU,CAACP,GAAX,CAAeiC,QAAf,EAAyB,IAAzB,CA9Ea;;AAAA;AAgFnBI,YAAAA,IAAI,CAACrD,OAAL;AAhFmB,8CAkFZ;AACLuB,cAAAA,UAAU,EAAVA,UADK;AAEL8B,cAAAA,IAAI,EAAJA;AAFK,aAlFY;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,GAAH;;AAAA,kBAAZL,YAAY;AAAA;AAAA;AAAA,GAAlB;;AAwFA,IAAMiB,WAAW;AAAA,uEAAG,kBAAO1E,OAAP,EAAgB2E,QAAhB,EAA0BxE,OAA1B;AAAA;AAAA;AAAA;AAAA;AAAA;AACZyE,YAAAA,KADY,GACJ,IAAInF,UAAJ,CAAe;AAC3BoF,cAAAA,IAAI,EAAE,IADqB;AAE3BnE,cAAAA,GAAG,EAAE,IAFsB;AAG3BW,cAAAA,MAAM,EAAE,IAHmB;AAI3ByD,cAAAA,SAAS,EAAE,IAJgB;AAK3BhB,cAAAA,IAAI,EAAE,EALqB;AAM3BiB,cAAAA,KAAK,EAAE,IANoB;AAO3BC,cAAAA,IAAI,EAAE;AAPqB,aAAf,EAQX7E,OARW,CADI;AAWTiE,YAAAA,CAXS,GAWL,CAXK;;AAAA;AAAA,kBAWFA,CAAC,GAAGO,QAAQ,CAAC9B,MAXX;AAAA;AAAA;AAAA;;AAAA;AAAA,mBAYV+B,KAAK,CAACK,OAAN,CAAcxD,GAAd,CAAkBkD,QAAQ,CAACP,CAAD,CAAR,CAAYc,IAA9B,EAAoC;AACxChC,cAAAA,IAAI,EAAEyB,QAAQ,CAACP,CAAD,CAAR,CAAYlB,IADsB;AAExCrB,cAAAA,GAAG,EAAE8C,QAAQ,CAACP,CAAD,CAAR,CAAYvC;AAFuB,aAApC,CAZU;;AAAA;AAWmBuC,YAAAA,CAAC,EAXpB;AAAA;AAAA;;AAAA;AAAA,8CAkBXtE,IAAI,CAAC8E,KAAK,CAAChD,KAAN,CAAY,EAAZ,EAAgB5B,OAAO,CAACwB,IAAxB,EAA8B,IAA9B,CAAD,CAlBO;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,GAAH;;AAAA,kBAAXkD,WAAW;AAAA;AAAA;AAAA,GAAjB;;AAqBAS,MAAM,CAACC,OAAP,GAAiB;AACf3B,EAAAA,YAAY,EAAZA,YADe;AAEf1D,EAAAA,mBAAmB,EAAnBA,mBAFe;AAGfgC,EAAAA,iBAAiB,EAAjBA,iBAHe;AAIfQ,EAAAA,oBAAoB,EAApBA,oBAJe;AAKfc,EAAAA,QAAQ,EAARA,QALe;AAMfqB,EAAAA,WAAW,EAAXA;AANe,CAAjB","sourcesContent":["'use strict'\n\nconst {\n  DAGNode\n} = require('ipld-dag-pb')\nconst Bucket = require('hamt-sharding/src/bucket')\nconst DirSharded = require('ipfs-unixfs-importer/src/dir-sharded')\nconst log = require('debug')('ipfs:mfs:core:utils:hamt-utils')\nconst UnixFS = require('ipfs-unixfs')\nconst mc = require('multicodec')\nconst mh = require('multihashes')\nconst last = require('async-iterator-last')\n\nconst updateHamtDirectory = async (context, links, bucket, options) => {\n  // update parent with new bit field\n  const data = Buffer.from(bucket._children.bitField().reverse())\n  const dir = new UnixFS('hamt-sharded-directory', data)\n  dir.fanout = bucket.tableSize()\n  dir.hashType = DirSharded.hashFn.code\n\n  const format = mc[options.format.toUpperCase().replace(/-/g, '_')]\n  const hashAlg = mh.names[options.hashAlg]\n\n  const parent = DAGNode.create(dir.marshal(), links)\n  const cid = await context.ipld.put(parent, format, {\n    cidVersion: options.cidVersion,\n    hashAlg,\n    hashOnly: !options.flush\n  })\n\n  return {\n    node: parent,\n    cid\n  }\n}\n\nconst recreateHamtLevel = async (links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hashFn: DirSharded.hashFn,\n    hash: parentBucket ? parentBucket._options.hash : undefined\n  }, parentBucket, positionAtParent)\n\n  if (parentBucket) {\n    parentBucket._putObjectAt(positionAtParent, bucket)\n  }\n\n  await addLinksToHamtBucket(links, bucket, rootBucket)\n\n  return bucket\n}\n\nconst addLinksToHamtBucket = async (links, bucket, rootBucket) => {\n  await Promise.all(\n    links.map(link => {\n      if (link.Name.length === 2) {\n        const pos = parseInt(link.Name, 16)\n\n        bucket._putObjectAt(pos, new Bucket({\n          hashFn: DirSharded.hashFn\n        }, bucket, pos))\n\n        return Promise.resolve()\n      }\n\n      return (rootBucket || bucket).put(link.Name.substring(2), {\n        size: link.TSize,\n        cid: link.Hash\n      })\n    })\n  )\n}\n\nconst toPrefix = (position) => {\n  return position\n    .toString('16')\n    .toUpperCase()\n    .padStart(2, '0')\n    .substring(0, 2)\n}\n\nconst generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateHamtLevel(rootNode.Links, null, null, null)\n  const position = await rootBucket._findNewBucketAndPos(fileName)\n\n  // the path to the root bucket\n  let path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }]\n  let currentBucket = position.bucket\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    })\n\n    currentBucket = currentBucket._parent\n  }\n\n  path.reverse()\n  path[0].node = rootNode\n\n  // load DAGNode for each path segment\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i]\n\n    // find prefix in links\n    const link = segment.node.Links\n      .filter(link => link.Name.substring(0, 2) === segment.prefix)\n      .pop()\n\n    // entry was not in shard\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`)\n      // return path\n      continue\n    }\n\n    // found entry\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`)\n      // file already existed, file will be added to the current bucket\n      // return path\n      continue\n    }\n\n    // found subshard\n    log(`Found subshard ${segment.prefix}`)\n    const node = await context.ipld.get(link.Hash)\n\n    // subshard hasn't been loaded, descend to the next level of the HAMT\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`)\n\n      await recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16))\n      const position = await rootBucket._findNewBucketAndPos(fileName)\n\n      // i--\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      })\n\n      continue\n    }\n\n    const nextSegment = path[i + 1]\n\n    // add intermediate links to bucket\n    await addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket)\n\n    nextSegment.node = node\n  }\n\n  await rootBucket.put(fileName, true)\n\n  path.reverse()\n\n  return {\n    rootBucket,\n    path\n  }\n}\n\nconst createShard = async (context, contents, options) => {\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: null,\n    parentKey: null,\n    path: '',\n    dirty: true,\n    flat: false\n  }, options)\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    })\n  }\n\n  return last(shard.flush('', context.ipld, null))\n}\n\nmodule.exports = {\n  generatePath,\n  updateHamtDirectory,\n  recreateHamtLevel,\n  addLinksToHamtBucket,\n  toPrefix,\n  createShard\n}\n"]},"metadata":{},"sourceType":"script"}